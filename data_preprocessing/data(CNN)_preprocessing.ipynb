{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    print('stories loading')\n",
    "    stories = list()\n",
    "\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        #print(filename)   #-> C:/Users/Desktop/dataset/cnn_stories/cnn/stories/0001d1afc246a7964130f43ae940af6bc6c57f01.story\n",
    "        \n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "        # print(doc)  -> 기사 내용 출력!\n",
    "        \n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        #print(story, highlights)\n",
    "        \n",
    "        # store 방법 2가지 (1.dictionary / 2. list)\n",
    "        stories.append({'story':story, 'highlights':highlights})\n",
    "        #test_stories.append(story)\n",
    "        #test_highlights.append(highlights)\n",
    "    return stories\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "    \n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    #print('highlight시작하는 index', index)\n",
    "    \n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    story= story.replace(\"\\n\\n\",\" \").strip()         #story 개행문자 제거 후 한문장으로 만들기\n",
    "    #print(len(highlights), highlights)               #highlights개수 + 1 (개행문자포함)\n",
    "    \n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]   #개행문자 제거!\n",
    "    #print(len(highlights), highlights)\n",
    "\n",
    "    # array형식으로 나눠져있는 문장데이터를 하나의 string으로 바꿈!\n",
    "    highlights_string = '. '.join(highlights)\n",
    "    highlights_string = highlights_string + '.'\n",
    "    #print(highlights_string,'\\n')\n",
    "    return story, highlights_string\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "# clean a list of lines\n",
    "def clean_lines_story(line):\n",
    "    \n",
    "    # 1. 일부러 앞문장에서만 찾고 --> 대부분 앞에 필요없는 데이터 있었음.(시간단축)\n",
    "    # 2. story 두가지 방식으로 전처리함\n",
    "    # 첫번째 '-- '   (다른단어) -- / (CNN) -- / 이름 or 지명 (CNN) -- \n",
    "    # 두번째 그냥 (CNN)\n",
    "    index = line.find('-- ',0,50)     #line 0~50까지 '-- ' 검색\n",
    "    index2 = line.find('(CNN)',0,50)  #line 0~50까지 CNN 검색\n",
    "    \n",
    "    # 검색결과 있을 때 그 위치의 index + (CNN) or '-- ' 길이만큼 이동\n",
    "    if index > -1:       #'-- ' 검색\n",
    "        line = line[index+len('-- '):] \n",
    "    elif index2 > -1:   #(CNN) 검색\n",
    "        line = line[index2+len('(CNN)'):] \n",
    "    else :              # 검색결과 없을 땐 그대로 출력!\n",
    "        pass\n",
    "        \n",
    "    return preprocess_sentence(line)\n",
    "    \n",
    "# clean a list of lines\n",
    "def clean_lines_highlight(line):\n",
    "    # 1. 일부러 앞문장에서만 찾고 --> 대부분 앞에 필요없는 데이터 있었음.(시간단축)\n",
    "    # 2. highlights 전처리\n",
    "    index3 = line.find('NEW:',0,10)        #line 0~10까지 NEW: 검색\n",
    "    \n",
    "    # 검색결과 있을 때 그 위치의 index + NEW: 길이만큼 이동\n",
    "    if index3 > -1:       #NEW: 검색\n",
    "        line = line[index3+len('NEW:'):]\n",
    "    else :              # 검색결과 없을 땐 그대로 출력!\n",
    "        pass\n",
    "    \n",
    "    return preprocess_sentence(line)\n",
    "  \n",
    "    \n",
    "def preprocess_sentence(line):\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
    "    line = re.sub(r'[\" \"]+', \" \", line)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    line = re.sub(r\"[^a-zA-Z0-9?.!,@%$¿]+\", \" \", line)\n",
    "\n",
    "    return line.strip().lower()\n",
    "    \n",
    "    \n",
    "# load stories\n",
    "#test_stories =[]\n",
    "#test_highlights = []\n",
    "directory = 'C:/Users/Jeyoung/Desktop/dataset/cnn_stories/cnn/stories'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "#print('Loaded test_stories %d' %len(test_stories), '/ Loaded test_highlights %d' %len(test_highlights))\n",
    "\n",
    "\n",
    "# clean stories (2가지 방법!!)\n",
    "#1. dictionary 형태 \n",
    "print('proprocessing')\n",
    "for s in stories:\n",
    "    s['story'] = clean_lines_story(s['story'])\n",
    "    s['highlights'] = clean_lines_highlight(s['highlights'])    \n",
    "    \n",
    "# 2. list 형태\n",
    "# for idx, value in enumerate(test_stories):\n",
    "#     test_stories[idx] = clean_lines(value)\n",
    "#     #print(value)\n",
    "\n",
    "df = pd.DataFrame(data = stories)\n",
    "print(df.describe())\n",
    "df.to_csv('cnn_stories_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중복값 체크1 (중복 제거 후 필요한만큼 데이터 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복체크 이전\n",
      "92465\n",
      "92465 \n",
      "\n",
      "story 중복체크 후 데이터 갯수\n",
      "89360\n",
      "89360 \n",
      "\n",
      "highlights 중복체크 후 데이터 갯수\n",
      "88640\n",
      "88640\n"
     ]
    }
   ],
   "source": [
    "duplicate_check = pd.read_csv('cnn_stories_all.csv').dropna()\n",
    "print('중복체크 이전')\n",
    "print(len(duplicate_check['story']))\n",
    "print(len(duplicate_check['highlights']), '\\n')\n",
    "\n",
    "duplicate_check = duplicate_check.drop_duplicates(subset = 'story')\n",
    "print('story 중복체크 후 데이터 갯수')\n",
    "print(len(duplicate_check['story']))\n",
    "print(len(duplicate_check['highlights']), '\\n')\n",
    "\n",
    "duplicate_check = duplicate_check.drop_duplicates(subset = 'highlights')\n",
    "print('highlights 중복체크 후 데이터 갯수')\n",
    "print(len(duplicate_check['story']))\n",
    "print(len(duplicate_check['highlights']))\n",
    "\n",
    "duplicate_check.to_csv('cnn_stories_all.csv', index = False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 데이터 랜덤하게 섞고 6만개 데이터 저장(train+vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88640\n"
     ]
    }
   ],
   "source": [
    "cnn_articles = pd.read_csv('cnn_stories_all.csv').dropna()\n",
    "print(len(cnn_articles['story']))   # 원래는 92579 --> nan 제거 후 92465  --> 중복값 제거 후 88640\n",
    "\n",
    "# The frac keyword argument specifies the fraction of rows to return in the random sample, so frac=1 means return all rows (in random order).\n",
    "# Here, specifying drop=True prevents .reset_index from creating a column containing the old index entries.\n",
    "cnn_articles = cnn_articles.sample(frac=1).reset_index(drop=True)\n",
    "cnn_articles = cnn_articles[:60000]\n",
    "cnn_articles.to_csv('cnn_stories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highlights    False\n",
       "story         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null check\n",
    "cnn_articles.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test 분류(5만, 1만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "cnn_articles = pd.read_csv('cnn_stories.csv')\n",
    "cnn_articles_train = cnn_articles[:50000]\n",
    "cnn_articles_train.to_csv('cnn_stories_train.csv', index=False)\n",
    "\n",
    "cnn_articles_test = cnn_articles[50000:]\n",
    "cnn_articles_test.to_csv('cnn_stories_test.csv', index=False)\n",
    "\n",
    "print(len(cnn_articles_train['highlights']))\n",
    "print(len(cnn_articles_test['story']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_articles_test.iloc[377])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 길이구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = pd.read_csv('cnn_stories_all.csv').dropna()\n",
    "\n",
    "story_count_list = []\n",
    "#for s in story['story']:\n",
    "#    c = len(str(s).split(' '))\n",
    "#    story_count_list.append(c)\n",
    "#    if c == 1:\n",
    "#        print(str(s).split(' '))\n",
    "        \n",
    "for s in story['story']:\n",
    "    story_count_list.append(len(str(s).split(' ')))\n",
    "print(min(story_count_list))\n",
    "print(max(story_count_list))\n",
    "print(sum(story_count_list)/len(story_count_list),'\\n')\n",
    "\n",
    "h_count_list = []\n",
    "for h in story['highlights']:\n",
    "    h_count_list.append(len(h.split(' ')))\n",
    "print(min(h_count_list))\n",
    "print(max(h_count_list))\n",
    "print(sum(h_count_list)/len(h_count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_index = []\n",
    "for i, count in enumerate(story_count_list):\n",
    "    if count == 1:\n",
    "        print(i)\n",
    "        weird_index.append(i)\n",
    "\n",
    "for i, s in enumerate(list(story['story'])):\n",
    "    if i in weird_index:\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jeyoung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story....\n",
      "highlights....\n",
      "csv...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "\n",
    "#train = pd.read_csv('train_data.csv').dropna()\n",
    "test = pd.read_csv('train_data.csv').dropna()\n",
    "new_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_story_list = []\n",
    "stopwords_highlight_list = []\n",
    "articles = list()\n",
    "\n",
    "print('story....')\n",
    "for t in test['story']:\n",
    "    word_tokens = t.split(\" \")     #띄어쓰기로 구분\n",
    "\n",
    "    tokenized = []\n",
    "    for w in word_tokens:\n",
    "        if w not in new_stopwords:\n",
    "            tokenized.append(w)\n",
    "    \n",
    "    #tokenized_story_list.append(len(tokenized))     평균구하기\n",
    "    story = ' '.join(tokenized)\n",
    "    stopwords_story_list.append(story)\n",
    "    \n",
    "\n",
    "\n",
    "print('highlights....')\n",
    "for t in test['highlights']:\n",
    "    word_tokens = t.split(\" \")   #띄어쓰기로 구분\n",
    "\n",
    "    tokenized = []\n",
    "    for w in word_tokens:\n",
    "        if w not in new_stopwords:\n",
    "            tokenized.append(w)\n",
    "            \n",
    "    #tokenized_highlight_list.append(len(tokenized))   평균구하기\n",
    "    highlight = ' '.join(tokenized)\n",
    "    stopwords_highlight_list.append(highlight)\n",
    "\n",
    "print('csv...')\n",
    "test_data_stopwords = pd.DataFrame(list(zip(stopwords_highlight_list, stopwords_story_list)), columns =['highlights', 'story']) \n",
    "test_data_stopwords.to_csv('train_data_stopwords.csv', index=False) \n",
    "    \n",
    "    \n",
    "    \n",
    "# stopwords 제거 후 평균값 구하기!   \n",
    "#print(tokenized_story_list)\n",
    "#print(tokenized_highlight_list)\n",
    "#print('story       -  max : ', max(tokenized_story_list), ' / min : ', min(tokenized_story_list), ' / mean : ', sum(tokenized_story_list)/len(tokenized_story_list))\n",
    "#print('highlights  -  max : ', max(tokenized_highlight_list), ' / min : ', min(tokenized_highlight_list), ' / mean : ', sum(tokenized_highlight_list)/len(tokenized_highlight_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
